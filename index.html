<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Hum to Lyrics â€“ Vocal & Lyric Rescue</title>
<style>
  body { font-family: system-ui, sans-serif; padding: 1rem; background: #0b0c10; color: #e5e7eb; }
  button, input { padding: 0.75rem 1rem; font-size: 1rem; border-radius: .5rem; border: none; }
  button { cursor: pointer; }
  .rec { background: #4f46e5; color: white; animation: pulse 1s infinite; }
  @keyframes pulse { 0%{box-shadow:0 0 0 0 rgba(79,70,229,.6);} 70%{box-shadow:0 0 0 20px rgba(79,70,229,0);} 100%{box-shadow:0 0 0 0 rgba(79,70,229,0);} }
  .search { margin-top: 1rem; display: flex; gap: 0.5rem; }
  input { flex: 1; border: 1px solid rgba(255,255,255,0.2); background: #111217; color: #e5e7eb; }
  pre { white-space: pre-wrap; background: #111217; padding: 1rem; border-radius: .5rem; margin-top: 1rem; }
</style>
</head>
<body>
<h1>Hum to Lyrics</h1>
<p id="status">Tap the mic, hum or sing for ~8 seconds.</p>
<label><input type="checkbox" id="lyricRescue" checked> Enable Lyric Rescue</label>
<button id="micBtn">ðŸŽ¤</button>
<div class="search">
  <input id="lyricQuery" placeholder="Or type a lyric snippetâ€¦">
  <button id="searchBtn">Search</button>
</div>
<pre id="output"></pre>
<script>
const micBtn = document.getElementById('micBtn');
const statusEl = document.getElementById('status');
const output = document.getElementById('output');
const lyricRescue = document.getElementById('lyricRescue');
const lyricQuery = document.getElementById('lyricQuery');
const searchBtn = document.getElementById('searchBtn');
let mediaRecorder, chunks = [];

async function recordOnce(ms=8000) {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  return new Promise((resolve, reject) => {
    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
    chunks = [];
    mediaRecorder.ondataavailable = e => chunks.push(e.data);
    mediaRecorder.onerror = e => reject(e.error || e);
    mediaRecorder.onstop = () => resolve(new Blob(chunks, { type: 'audio/webm' }));
    mediaRecorder.start();
    setTimeout(() => { if (mediaRecorder.state === 'recording') mediaRecorder.stop(); }, ms);
  });
}

async function findLyricsByText(q) {
  const res = await fetch(`/api/audd-lyrics?q=${encodeURIComponent(q)}`);
  if (!res.ok) throw new Error('Lyrics lookup failed');
  const json = await res.json();
  if (!json.result || !json.result.length) throw new Error('No lyrics found');
  json.result.sort((a,b)=>(b.lyrics?.length||0)-(a.lyrics?.length||0));
  return json.result[0];
}

async function lyricRescueListen(ms=8000) {
  const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
  if (!SR) throw new Error('Speech recognition not supported');
  return new Promise((resolve, reject) => {
    const r = new SR();
    let transcript = '';
    r.continuous = false;
    r.interimResults = true;
    r.lang = navigator.language || 'en-US';
    const timer = setTimeout(()=>{ try{ r.stop(); }catch{} }, ms);
    r.onresult = e => {
      for (let i = e.resultIndex; i < e.results.length; i++) {
        if (e.results[i].isFinal) transcript += e.results[i][0].transcript + ' ';
      }
      statusEl.textContent = 'Heard: ' + transcript.trim();
    };
    r.onerror = e => { clearTimeout(timer); reject(new Error(e.error||'Speech error')); };
    r.onend = () => { clearTimeout(timer); resolve(transcript.trim()); };
    try { r.start(); statusEl.textContent = 'Listening for lyricsâ€¦'; }
    catch(err){ clearTimeout(timer); reject(err); }
  });
}

async function identify(blob) {
  try {
    const res = await fetch('/api/audd', { method: 'POST', headers: { 'Content-Type': 'application/octet-stream' }, body: blob });
    if (res.ok) {
      const data = await res.json();
      if (data && data.result) return data.result;
    }
  } catch(e) {
    console.warn('AudD failed', e);
  }
  if (lyricRescue.checked) {
    try {
      const heard = await lyricRescueListen(8000);
      if (heard.split(/\s+/).length >= 3) {
        return await findLyricsByText(heard);
      }
    } catch(e) {
      console.warn('Lyric rescue failed', e);
    }
  }
  return { title: 'No match', artist: '', lyrics: 'Nothing found' };

  // Server STT fallback: send raw blob to Whisper, then search lyrics
try {
  const fd = new FormData();
  fd.append('file', blob, 'clip.webm'); // content-type auto-set by browser
  const tRes = await fetch('/api/transcribe', { method: 'POST', body: fd });
  if (tRes.ok) {
    const tData = await tRes.json();
    if (tData?.text) {
      const l = await findLyricsByText(tData.text);
      return l; // { title, artist, lyrics, ... }
    }
  }
} catch (e) {
  console.warn('Whisper fallback failed', e);
}

}

async function go() {
  try {
    micBtn.classList.add('rec');
    statusEl.textContent = 'Recordingâ€¦';
    const blob = await recordOnce();
    micBtn.classList.remove('rec');
    statusEl.textContent = 'Processingâ€¦';
    const result = await identify(blob);
    output.textContent = JSON.stringify(result, null, 2);
    statusEl.textContent = 'Done';
  } catch (err) {
    console.error(err);
    statusEl.textContent = 'Error: ' + err.message;
  }
}

searchBtn.addEventListener('click', async () => {
  const q = lyricQuery.value.trim();
  if (!q) return;
  statusEl.textContent = 'Searching lyricsâ€¦';
  try {
    const r = await findLyricsByText(q);
    output.textContent = JSON.stringify(r, null, 2);
    statusEl.textContent = 'Found by lyrics.';
  } catch(e) {
    statusEl.textContent = 'No lyrics found.';
  }
});

micBtn.onclick = () => {
  if (mediaRecorder && mediaRecorder.state === 'recording') {
    mediaRecorder.stop();
  } else {
    go();
  }
};
</script>
</body>
</html>

